{
  "hash": "9fd9ed4452122b86ebebda39283965af",
  "result": {
    "markdown": "---\nformat: \n  clean-revealjs:\n    footer: \"USNCCM 17 -- July 2023\"\n    scrollable: false\n    touch: true\nexecute:\n  freeze: true\ntitle: Multi-Output Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Uncertainties\nauthor: \n  - name: Mingyuan Yang \n    email: mingyuanyang@pku.edu.cn \n    affiliations: \n      - \"Peking University\"\n      - \"The University of Texas at Austin\"\n  - name: John T. Foster\n    email: john.foster@utexas.edu\n    affiliations: \n      - \"Oden Institute for Computational Engineering and Science\"\n      - \"Hildebrand Department of Petroleum and Geosystems Engineering\"\n      - \"Department of Aerospace Engineering and Engineering Mechanics\"\n      - \"The University of Texas at Austin\"\ndate: July 26, 2023\nbibliography: bib_files/usnccm17_mopinn.bib \n---\n\n\n## Physics Informed Neural Networks\n### PINNs\n\nIntroduced in [@raissi2019physics] as a general method for solving partial differential equations.\n\nAlready recieved >5900 citations since posting on arXiv in 2018!\n\n## Generic PINN architecture\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](usnccm17_mopinn_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## Loss function for generic PINN system {.smaller}\n\n$$\n\\begin{align}\n    r_d(u_{NN}, f_{NN}) &= \\mathcal{L} u_{NN} - f_{NN},\\qquad x \\in \\Omega\n    \\\\\n    r_e(u_{NN}) &= u_{NN} - h,\\qquad x \\in \\partial \\Omega_h\n    \\\\\n    r_n(u_{NN}) &= \\frac{\\partial u_{NN}}{\\partial x} - g,\\qquad x \\in \\partial \\Omega_g \n    \\\\\n    r_{u}(u_{NN}) &= u_{NN}(x^u_i) - u_m(x^u_i), \\qquad i=1,2,...,n \n    \\\\\n    r_{f}(f_{NN}) &= f_{NN}(x^f_i) - f_m(x^f_i), \\qquad i=1,2,...,m \n\\end{align}\n$$\n\n::: {.fragment}\n$$\n\\begin{gather}\n    L_{MSE} = \\frac{1}{N}\\sum_{i}^N r_d^2 + \\frac{1}{N_e}\\sum_{i=1}^{N_e} r_e^2 + \\frac{1}{N_n}\\sum_{i=1}^{N_n} r_n^2 + \\frac{1}{n}\\sum_{i=1}^{n} r_{u}^2 + \\frac{1}{m}\\sum_{i=1}^m r_{f}^2\n\\end{gather}\n$$\n:::\n\n## Extensions of PINNs for UQ\n\n* B-PINNs [@yang2021b]\n* E-PINN [@jiang2022pinn]\n\n\n## Multi-Output PINN \n### MO-PINN [@mingyuan2022a]\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](usnccm17_mopinn_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Loss function for generic MO-PINN system {.smaller}\n\n$$\n\\begin{align}\n    r_d(u_{NN}^j, f_{NN}^j) &= \\mathcal{L} u_{NN}^j - f_{NN}^j,\\qquad x \\in \\Omega\n    \\\\\n    r_e(u_{NN}^j) &= u_{NN}^j - h,\\qquad x \\in \\partial \\Omega_h\n    \\\\\n    r_n(u_{NN}^j) &= \\frac{\\partial u_{NN}^j}{\\partial x} - g,\\qquad x \\in \\partial \\Omega_g\n    \\\\\n    r_{um}(u_{NN}^j) &= u_{NN}^j(x^u_i) - \\left(u_m(x^u_i) + \\sigma_u^j\\right), \\qquad i=1,2,...,n \n    \\\\\n    r_{fm}(f_{NN}^j) &= f_{NN}^j(x^f_i) - \\left(f_m(x^f_i) + \\sigma_f^j\\right), \\qquad i=1,2,...,m \\\n\\end{align}\n$$\n\n::: {.fragment}\n$$\n\\begin{gather}\n    L_{MSE} = \\frac{1}{M}\\sum_{j=1}^{M} \\left( \\frac{1}{N}\\sum_{i}^N r_d^2 + \\frac{1}{N_e}\\sum_{i=1}^{N_e} r_e^2 + \\frac{1}{N_n}\\sum_{i=1}^{N_n} r_n^2 + \\frac{1}{n}\\sum_{i=1}^{n} r_{um}^2 + \\frac{1}{m}\\sum_{i=1}^m r_{fm}^2  \\right)\n\\end{gather}\n$$\n:::\n# Forward PDE problems\n\n---\n\n\n## One-dimensional linear Poisson equation\n\n$$\n\\begin{gathered}\n    \\lambda \\frac{\\partial^2 u}{\\partial x^2} = f, \\qquad x \\in [-0.7, 0.7]\n\\end{gathered}\n$$ \nwhere $\\lambda = 0.01$ and  $u=\\sin^3(6x)$\n\n::: {layout=\"[[1, 1]]\"}\n\n![Solution $u$](img_mopinn/1d_linear_solution_u.png)\n\n![Source $f$ from manufactured solution](img_mopinn/1d_linear_source_f.png)\n\n:::\n\n---\n\n## Network architecture and hyperparameters\n\n * 2 neural networks: $u_{NN}$ and $f_{NN}$\n\n * 2 hidden layers with 20 and 40 neurons each\n\n * $\\tanh$ activation function\n\n * ADAM optimizer \n\n * $10^{-3}$ learning rate\n\n * Xavier normalization\n\n * 10000 epochs\n\n * 500 outputs\n\n\n---\n\n## Predictions w/ $\\sigma = 0.01$ noise on measurements\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$ w/ raw solutions](img_mopinn/1d_linear_u_raw_0.01_noise.png){width=350px}\n\n![Prediction $f$ w/ raw solutions](img_mopinn/1d_linear_f_raw_0.01_noise.png){width=350px}\n\n![Prediction $u$ w/ $2\\sigma$ distribution](img_mopinn/1d_linear_u_with_std_0.01_noise.png){width=350px}\n\n![Prediction $f$ w/ $2\\sigma$ distribution](img_mopinn/1d_linear_f_with_std_0.01_noise.png){width=350px}\n\n:::\n\n---\n\n## Predictions w/ $\\sigma = 0.1$ noise on measurements\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$ w/ raw solutions](img_mopinn/1d_linear_u_raw_0.1_noise.png){width=350px}\n\n![Prediction $f$ w/ raw solutions](img_mopinn/1d_linear_f_raw_0.1_noise.png){width=350px}\n\n![Prediction $u$ w/ $2\\sigma$ distribution](img_mopinn/1d_linear_u_with_std_0.1_noise.png){width=350px}\n\n![Prediction $f$ w/ $2\\sigma$ distribution](img_mopinn/1d_linear_f_with_std_0.1_noise.png){width=350px}\n\n:::\n\n---\n\n## Sensitivity to random network parameter initialization \n### $\\sigma = 0.1$ noise\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![](img_mopinn/1d_linear_u_with_std_run_1.png){width=350px}\n\n![](img_mopinn/1d_linear_u_with_std_run_2.png){width=350px}\n\n![](img_mopinn/1d_linear_u_with_std_run_3.png){width=350px}\n\n![](img_mopinn/1d_linear_u_with_std_run_4.png){width=350px}\n\n:::\n\n---\n\n\n## Sensitivity to measurement sampling \n### $\\sigma = 0.1$ noise\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![](img_mopinn/1d_linear_u_with_std_data_1.png){width=400px}\n\n![](img_mopinn/1d_linear_u_with_std_data_2.png){width=400px}\n\n![](img_mopinn/1d_linear_u_with_std_data_3.png){width=400px}\n\n![](img_mopinn/1d_linear_u_with_std_data_4.png){width=400px}\n\n:::\n\n## One-dimensional nonlinear Poisson equation\n\n$$\n\\begin{gathered}\n    \\lambda \\frac{\\partial^2 u}{\\partial x^2} + k \\tanh(u) = f, \\qquad x \\in [-0.7, 0.7]\n\\end{gathered}\n$$ \nwhere $\\lambda = 0.01, k=0.7$ and  $u=\\sin^3(6x)$\n\n::: {layout=\"[[1, 1]]\"}\n\n![Solution $u$](img_mopinn/1d_nonlinear_solution_u.png)\n\n![Source $f$ from manufactured solution](img_mopinn/1d_nonlinear_source_f.png)\n\n:::\n\n---\n\n## Predictions w/ $\\sigma = 0.01$ noise on measurements\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$ w/ raw solutions](img_mopinn/1d_nonlinear_u_raw_0.01_noise.png){width=350px}\n\n![Prediction $f$ w/ raw solutions](img_mopinn/1d_nonlinear_f_raw_0.01_noise.png){width=350px}\n\n![Prediction $u$ w/ $2\\sigma$ distribution](img_mopinn/1d_nonlinear_u_with_std_0.01_noise.png){width=350px}\n\n![Prediction $f$ w/ $2\\sigma$ distribution](img_mopinn/1d_nonlinear_f_with_std_0.01_noise.png){width=350px}\n\n:::\n\n---\n\n## Predictions w/ $\\sigma = 0.1$ noise on measurements\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$ w/ raw solutions](img_mopinn/1d_nonlinear_u_raw_0.1_noise.png){width=350px}\n\n![Prediction $f$ w/ raw solutions](img_mopinn/1d_nonlinear_f_raw_0.1_noise.png){width=350px}\n\n![Prediction $u$ w/ $2\\sigma$ distribution](img_mopinn/1d_nonlinear_u_with_std_0.1_noise.png){width=350px}\n\n![Prediction $f$ w/ $2\\sigma$ distribution](img_mopinn/1d_nonlinear_f_with_std_0.1_noise.png){width=350px}\n\n:::\n\n--- \n\n## Two-dimensional nonlinear Allen-Cahn equation\n\n$$\n\\begin{gathered}\n    \\lambda \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) + u\\left(u^2 -1 \\right) = f, \\qquad x,y \\in [-1, 1]\n\\end{gathered}\n$$ \nwhere $\\lambda = 0.01$ and  $u=\\sin(\\pi x)\\sin(\\pi y)$\n\n::: {layout=\"[[1, 1]]\"}\n\n![Solution $u$](img_mopinn/2d_ac_u_data.png)\n\n![Source $f$ from manufactured solution](img_mopinn/2d_ac_f_data.png)\n\n:::\n\n---\n\n## Network architecture and hyperparameters\n\n * 2 neural networks: $u_{NN}$ and $f_{NN}$\n\n * 3 hidden layers with 200 neurons each\n\n * $\\tanh$ activation function\n\n * ADAM optimizer \n\n * $10^{-3}$ learning rate\n\n * Xavier normalization\n\n * 50000 epochs\n\n * 2000 outputs\n\n\n---\n\n## Predictions w/ $\\sigma = 0.01$ noise on measurements\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$](img_mopinn/2d_ac_0.01_noise_u.png){width=300px}\n\n![$L_2$ error](img_mopinn/2d_ac_0.01_noise_u_error.png){width=300px}\n\n![Standard deviation of predictions](img_mopinn/2d_ac_0.01_noise_u_std.png){width=300px}\n\n![Bounded by 2$\\sigma$ -- *red = bounded, blue = not bounded*](img_mopinn/2d_ac_0.01_noise_2_stds.png){width=250px}\n\n:::\n\n---\n\n## Predictions w/ $\\sigma = 0.1$ noise on measurements\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$](img_mopinn/2d_ac_0.1_noise_u.png){width=300px}\n\n![$L_2$ error](img_mopinn/2d_ac_0.1_noise_u_error.png){width=300px}\n\n![Standard deviation of predictions](img_mopinn/2d_ac_0.1_noise_u_std.png){width=300px}\n\n![Bounded by 2$\\sigma$ -- *red = bounded, blue = not bounded*](img_mopinn/2d_ac_0.1_noise_2_stds.png){width=250px}\n\n:::\n\n# Inverse Problems\n\n---\n\n## One-dimensional nonlinear Poisson equation\n\n$$\n\\begin{gathered}\n    \\lambda \\frac{\\partial^2 u}{\\partial x^2} + k \\tanh(u) = f, \\qquad x \\in [-0.7, 0.7]\n\\end{gathered}\n$$ \nwhere $\\lambda = 0.01$.\n\n$k=[???, ???, ???, \\dots, ???]$ with $N$ entries corresponding to $N$ outputs of the MO-PINN\n\n---\n\n## Predictions \n### $u$ and $f$\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![Prediction $u$ w/ $\\sigma=0.01$ noise](img_mopinn/1d_inverse_u_0.01_noise.png){width=350px}\n\n![Prediction $u$ w/ $\\sigma=0.1$ noise](img_mopinn/1d_inverse_u_0.1_noise.png){width=350px}\n\n![Prediction $f$ w/ $\\sigma=0.01$ noise](img_mopinn/1d_inverse_f_0.01_noise.png){width=350px}\n\n![Prediction $f$ w/ $\\sigma=0.1$ noise](img_mopinn/1d_inverse_f_0.1_noise.png){width=350px}\n\n:::\n\n---\n\n## Inverse Estimates \n### $k_{exact} = 0.7$\n\n::: {layout-ncol=2}\n\n![$\\sigma=0.01$ noise, $k_{avg} = 0.698$](img_mopinn/1d_inverse_k_distribution_0.01_noise.png){width=350px}\n\n![$\\sigma=0.1$ noise, $k_{avg} = 0.678$](img_mopinn/1d_inverse_k_distribution_0.1_noise.png){width=350px}\n\n:::\n\n---\n\n## Sensitivity of $k_{avg}$ w.r.t number of outputs \n### $\\sigma=0.1$ noise\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![10 outputs, $k_{avg} = 0.67$](img_mopinn/1d_inverse_k_10.png){width=350px}\n\n![50 outputs, $k_{avg} = 0.684$](img_mopinn/1d_inverse_k_50.png){width=350px}\n\n![100 outputs, $k_{avg} = 0.668$](img_mopinn/1d_inverse_k_100.png){width=350px}\n\n![500 outputs, $k_{avg} = 0.673$](img_mopinn/1d_inverse_k_500.png){width=350px}\n\n:::\n\n---\n\n## Two-dimensional Allen-Cahn Equation \n\n$$\n\\begin{gathered}\n    \\lambda \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) + u\\left(u^2 -1 \\right) = f, \\qquad x,y \\in [-1, 1]\n\\end{gathered}\n$$ \nwhere $\\lambda = 0.01$ and  $u=\\sin(\\pi x)\\sin(\\pi y)$\n\n\n$k=[???, ???, ???, \\dots, ???]$ with $N$ entries corresponding to $N$ outputs of the MO-PINN\n\n::: {layout-ncol=2}\n\n![Solution $u$ and measurements](img_mopinn/2d_inverse_u.png){width=350px}\n\n![Solution $f$ and measurements](img_mopinn/2d_inverse_f.png){width=350px}\n\n:::\n\n---\n\n## Inverse Estimates\n### $k_{exact} = 1.0$\n\n::: {layout-ncol=2}\n\n![$\\sigma=0.01$ noise,  $k_{avg} = 0.995$](img_mopinn/2d_inverse_k_0.01_noise.png){width=350px}\n\n![$\\sigma=0.1$ noise,  $k_{avg} = 1.02$](img_mopinn/2d_inverse_k_0.1_noise.png){width=350px}\n\n:::\n\n# Incorporating prior statistical knowledge\n\n\n---\n\n## Comparison to Monte Carlo FEM \n### One-dimensional linear Poisson equation\n\n::: {layout=\"[[1, 1], [1, 1]]\"}\n\n![MO-PINN prediction of $u$](img_mopinn/validation_u_nn.png){width=350px}\n\n![FEA prediction of $u$](img_mopinn/validation_u_fenics.png){width=350px}\n\n![MO-PINN prediction of $f$](img_mopinn/validation_f_nn.png){width=350px}\n\n![FEA prediction of $f$](img_mopinn/validation_f_fenics.png){width=350px}\n\n:::\n\n---\n\n## Comparison of distributions\n### MO-PINN vs. FEA Monte Carlo\n\n![Quantile-quantile plot of $u$ at 9 locations](img_mopinn/quantile.png)\n\n---\n\n## $u$ predictions with only 5 measurements \n### Using mean and std to enhance learning \n\n::: {layout-ncol=3}\n\n![Only 5 measurements](img_mopinn/nn_alone_u.png){width=350px}\n\n![5 measurements and mean](img_mopinn/integration_only_mean_u){width=350px}\n\n![5 measurements, mean, and std](img_mopinn/integration_u.png){width=350px}\n\n:::\n\n---\n\n## $f$ predictions with only 5 measurements \n### Using mean and std to enhance learning \n\n::: {layout-ncol=3}\n\n![Only 5 measurements](img_mopinn/nn_alone_f.png){width=350px}\n\n![5 measurements and mean](img_mopinn/integration_only_mean_f){width=350px}\n\n![5 measurements, mean, and std](img_mopinn/integration_f.png){width=350px}\n\n:::\n\n---\n\n## Conclusions\n\n\n* MO-PINNs appear promising for UQ\n* MO-PINNs can learn solution, source terms, and parameters simultaneously\n* MO-PINNs are faster than Monte Carlo forward solutions for the problem studied\n  * Only need to train a single network\n\n---\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "usnccm17_mopinn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}