{
  "hash": "00c534b5d729b206b3894a720621286b",
  "result": {
    "markdown": "---\nformat: \n  clean-revealjs:\n    footer: \"USNCCM 17 -- July 2023\"\n    scrollable: true\nexecute:\n  freeze: true\ntitle: Multi-Output Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Uncertainties\nauthor: \n  - name: Mingyuan Yang \n    email: mingyuanyang@pku.edu.cn \n    affiliations: \n      - \"Peking University\"\n      - \"The University of Texas at Austin\"\n  - name: John T. Foster\n    email: john.foster@utexas.edu\n    affiliations: \n      - \"Oden Institute for Computational Engineering and Science\"\n      - \"Hildebrand Department of Petroleum and Geosystems Engineering\"\n      - \"Department of Aerospace Engineering and Engineering Mechanics\"\n      - \"The University of Texas at Austin\"\ndate: July 26, 2023\n---\n\n\n\n## PINN\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](usnccm17_mopinn_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Multi-Output PINN \n### MO-PINN\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](usnccm17_mopinn_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n# Forward PDE problems\n\n---\n\n\n## One-dimensional linear Poisson equation\n\n$$\n\\begin{gathered}\n    \\lambda \\frac{\\partial^2 u}{\\partial x^2} = f, \\qquad x \\in [-0.7, 0.7]\n\\end{gathered}\n$$ \nwhere $\\lambda = 0.01$ and  $u=\\sin^3(6x)$\n\n:::: {layout=\"[[1, 1]]\"}\n\n![Solution $u$](img_mopinn/1d_linear_solution_u.png)\n\n![Source $f$ from manufactured solution](img_mopinn/1d_linear_source_f.png)\n\n::::\n\n---\n\n## Network architecture and hyperparameters\n\n * 2 neural networks: $u_{NN}$ and $f_{NN}$\n\n * 2 hidden layers with 20 and 40 neurons each\n\n * $\\tanh$ activation function\n\n * ADAM optimizer \n\n * $10^{-3}$ learning rate\n\n * Xavier normalization\n\n * 10000 epochs\n\n * 500 outputs\n\n\n---\n\n## Predictions w/ $\\sigma = 0.01$ noise on measurements\n\n::: {layout-nrow=2}\n\n![Prediction $u$ w/ raw solutions](img_mopinn/1d_linear_u_raw_0.01_noise.png){height=40%}\n\n![Prediction $f$ w/ raw solutions](img_mopinn/1d_linear_f_raw_0.01_noise.png){height=40%}\n\n![Prediction $u$ w/ $2\\sigma$ distribution](img_mopinn/1d_linear_u_with_std_0.01_noise.png){height=40%}\n\n![Prediction $f$ w/ $2\\sigma$ distribution](img_mopinn/1d_linear_f_with_std_0.01_noise.png){height=40%}\n\n:::\n\n\n<!---->\n<!-- Now we assume that the functional form of $u$ and $f$ are both unknown, -->\n<!-- and only sparse noisy measurements of $u$ and $f$ are available as -->\n<!-- training data. The goal of training is to obtain two neural network -->\n<!-- representations for $u$ and $f$ over the whole domain. -->\n<!---->\n<!-- Here we assume that 16 measurements of $f$ are available which are -->\n<!-- equally sampled in the range $x \\in [-0.7, 0.7]$. Additionally, we -->\n<!-- assume that two measurements of $u$ at two ends are available, which -->\n<!-- serve as the Dirichlet boundary conditions for the equation. We also -->\n<!-- assume that both measurements of $u$ and $f$ are noisy and the errors -->\n<!-- follow a given distribution. In this example, we investigate two cases -->\n<!-- where errors follow the Gaussian distribution with different standard -->\n<!-- deviations: -->\n<!---->\n<!-- ::: center -->\n<!-- 1.  $\\epsilon_u \\sim \\mathcal{N}(0,\\,0.01^{2})\\,,\\qquad \\epsilon_f \\sim \\mathcal{N}(0,\\,0.01^{2}),$ -->\n<!---->\n<!-- 2.  $\\epsilon_u \\sim \\mathcal{N}(0,\\,0.1^{2})\\,,\\qquad \\epsilon_f \\sim \\mathcal{N}(0,\\,0.1^{2}),$ -->\n<!-- ::: -->\n<!---->\n<!-- The training data of $u_{data}$ and $f_{data}$ is then -->\n<!-- $$\\begin{gathered} -->\n<!--     u_{data} = u_{exact} + \\epsilon_u \\label{u data},  -->\n<!--     \\\\ -->\n<!--     f_{data} = f_{exact} + \\epsilon_f \\label{f data}. -->\n<!-- \\end{gathered}$$ Consequently, the multiple outputs of neural networks -->\n<!-- are $$\\begin{gathered} -->\n<!--     u_{output} = u_{data} + \\epsilon_u, \\notag -->\n<!--     \\\\ -->\n<!--     f_{output} = f_{data} + \\epsilon_f. \\notag -->\n<!-- \\end{gathered}$$ -->\n<!---->\n<!-- In this work, all the examples of MO-PINNs are implemented with PyTorch -->\n<!-- [@paszke2017automatic]. The neural network structures are the same for -->\n<!-- $u_{NN}$ and $f_{NN}$, which are defined as fully connected neural -->\n<!-- networks with two hidden layers consisting of 20 and 40 neurons -->\n<!-- individually while the output layer consists of 500 neurons. The -->\n<!-- hyperbolic tangent function ($\\tanh{}$) is used as activation function -->\n<!-- in this section and throughout the rest of this work. The ADAM optimizer -->\n<!-- is used for training with a learning rate of $10^{-3}$. The Xavier -->\n<!-- normal initialization strategy is used in this study which it yields -->\n<!-- neural networks with different weights at the beginning of training for -->\n<!-- different random seeds. All results in this section were obtained after -->\n<!-- 10000 epochs of training. -->\n<!---->\n<!-- The results of the two cases with different noise scales are shown in -->\n<!-- Figure -->\n<!-- [\\[fig:1d linear possion u and f prediction 0.01 noise\\]](#fig:1d linear possion u and f prediction 0.01 noise){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d linear possion u and f prediction 0.01 noise\"} and -->\n<!-- Figure -->\n<!-- [\\[fig:1d linear possion u and f prediction 0.1 noise\\]](#fig:1d linear possion u and f prediction 0.1 noise){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d linear possion u and f prediction 0.1 noise\"}. First -->\n<!-- we see that the means of predictions for both $u$ and $f$ pass through -->\n<!-- the noisy measurements which is consistent with our error distribution -->\n<!-- assumptions. Next we see that the multiple outputs from neural networks -->\n<!-- form the posterior estimations of $u$ and $f$ over the investigated -->\n<!-- range $[-0.7, 0.7]$. In addition, it can be seen that the exact -->\n<!-- solutions are bounded by two standard deviations, which means that the -->\n<!-- predictions are reasonable. Last, we see that the standard deviations of -->\n<!-- predictions grow as the measurement error increases. The results that we -->\n<!-- obtain with MO-PINN are comparable with the ones presented in paper -->\n<!-- [@yang2021b] using Bayesian Hamiltonian Monte Carlo physics-informed -->\n<!-- neural networks (B-PINN-HMC). However, our approach does not require -->\n<!-- assumed prior distributions of hyperparameters in neural networks and -->\n<!-- can be easily implemented by modifying the standard PINN. -->\n<!---->\n<!-- We also verified our solutions from multiple aspects. First, we checked -->\n<!-- the consistency of the solutions when the two neural networks are -->\n<!-- initialized differently and different random noise are assigned to the -->\n<!-- outputs. Here we take the case 2 as example and run the simulations with -->\n<!-- different random seeds while keeping the measurements the same. Four -->\n<!-- predictions of $u$ are presented in Figure -->\n<!-- [\\[fig:1d linear possion u with different random seeds\\]](#fig:1d linear possion u with different random seeds){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d linear possion u with different random seeds\"}. It is -->\n<!-- shown that the results are in agreement which means that the solution is -->\n<!-- consistent and invariant to the randomness at the beginning of training. -->\n<!---->\n<!-- It should also be noted that the predictions are different with respect -->\n<!-- to different noisy measurements from -->\n<!-- Equation [\\[u data\\]](#u data){reference-type=\"eqref\" -->\n<!-- reference=\"u data\"} and -->\n<!-- Equation [\\[f data\\]](#f data){reference-type=\"eqref\" -->\n<!-- reference=\"f data\"}. In an physical setting, we would likely have no -->\n<!-- control over the measurements in practice, which means that the best we -->\n<!-- can expect is that the exact solution is bounded by two or three -->\n<!-- standard deviations for all possible measurements. Therefore, we tested -->\n<!-- our approach on different measurements and the results are shown in -->\n<!-- Figure [\\[fig:1d linear possion u with different measurements\\]](#fig:1d linear possion u with different measurements){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d linear possion u with different measurements\"}. We can -->\n<!-- see that for all cases the exact solution is bounded by two standard -->\n<!-- deviations which confirms the reliability of our approach. -->\n<!---->\n<!-- Finally, we also tested the convergence of the predictions with respect -->\n<!-- to different number of outputs used in the neural networks. In this -->\n<!-- work, we do not have a rigorous theory to derive the optimal number of -->\n<!-- outputs so that they can truly represent the posterior distributions. -->\n<!-- Instead, an ad hoc strategy is adopted here such that we incrementally -->\n<!-- increased the number of outputs and accepted the results when the change -->\n<!-- from previous iterations was trivial. For example, in Figure -->\n<!-- [\\[fig:1d linear possion u with different number of outputs on the same measurements.\\]](#fig:1d linear possion u with different number of outputs on the same measurements.){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d linear possion u with different number of outputs on the same measurements.\"}, -->\n<!-- we tested our approach with different number of outputs for both $u$ and -->\n<!-- $f$ neural networks on the same measurements. It is shown that the mean -->\n<!-- of predictions as well as the shady areas do not change much from 50 -->\n<!-- outputs to 100 and 500 outputs, so that we consider the predictions with -->\n<!-- 500 outputs to be our converged solution to this problem. -->\n<!---->\n<!-- ### 1D nonlinear Poisson equation {#1d nonlinear case} -->\n<!---->\n<!-- In this section we consider the following one-dimensional nonlinear -->\n<!-- Poisson equation $$\\begin{gathered} -->\n<!--     \\lambda \\frac{\\partial^2 u}{\\partial x^2} + k\\tanh(u) = f, \\qquad x \\in [-0.7, 0.7], \\label{1d nonlinear possion equation}  -->\n<!-- \\end{gathered}$$ where $\\lambda = 0.01$, $k=0.7$ and the solution $u$ is -->\n<!-- assumed to be the same with the one in the last section, -->\n<!-- i.e. $u=\\sin^3(6x)$. Therefore, the source term $f$ can be derived from -->\n<!-- the -->\n<!-- Equation [\\[1d nonlinear possion equation\\]](#1d nonlinear possion equation){reference-type=\"eqref\" -->\n<!-- reference=\"1d nonlinear possion equation\"} with manufactured solution. -->\n<!-- The solution $u$ and the exact source term $f$ are shown in Figure -->\n<!-- [\\[fig:1d nonlinear possion u and f\\]](#fig:1d nonlinear possion u and f){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d nonlinear possion u and f\"}. -->\n<!---->\n<!-- This time we assume that 32 measurements of $f$ are available to us -->\n<!-- which are equidistantly sampled in the range $x \\in [-0.7, 0.7]$, while -->\n<!-- the exact expression of $f$ is assumed to be unknown. Again, we assume -->\n<!-- that two measurements of $u$ at two ends are available, which serve as -->\n<!-- the Dirichlet boundary conditions for the equation. We also investigate -->\n<!-- two cases with errors following the same distribution as in -->\n<!-- § [3.1.1](#linear case){reference-type=\"ref\" reference=\"linear case\"}. -->\n<!-- The neural network structures and the settings of training are kept the -->\n<!-- same as in last section. All results are collected after 10000 epochs. -->\n<!---->\n<!-- The predictions of $u$ and $f$ for the two cases are shown -->\n<!-- Figure [\\[fig:1d nonlinear possion u and f prediction 0.01 noise\\]](#fig:1d nonlinear possion u and f prediction 0.01 noise){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d nonlinear possion u and f prediction 0.01 noise\"} and -->\n<!-- Figure [\\[fig:1d nonlinear possion u and f prediction 0.1 noise\\]](#fig:1d nonlinear possion u and f prediction 0.1 noise){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d nonlinear possion u and f prediction 0.1 noise\"}. -->\n<!-- Again, we have the same observations about the results as in the last -->\n<!-- section that exact solution of $u$ is bounded by two standard deviations -->\n<!-- and the shaded area grows as errors from the measurements increase. -->\n<!-- Nevertheless, it is shown that the standard deviations of $u$ -->\n<!-- predictions are smaller than those in the linear Poisson example. This -->\n<!-- is due to the fact that there are more measurements of $f$ available for -->\n<!-- training in this example which reduces the uncertainty of predictions. -->\n<!-- This observation is also consistent with the results presented in -->\n<!-- [@yang2021b]. -->\n<!---->\n<!-- ### 2D nonlinear Allen-Cahn equation {#2d forward section} -->\n<!---->\n<!-- In this section we test the MO-PINN on a two-dimensional problem with -->\n<!-- the nonlinear Allen-Cahn equation. The equation can be written as -->\n<!-- $$\\begin{gathered} -->\n<!--     \\lambda \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right) + u\\left(u^2-1\\right) = f, \\qquad x,y \\in [-1, 1], \\label{2d ac equation}  -->\n<!-- \\end{gathered}$$ where $\\lambda$ is a constant of 0.01 in this example. -->\n<!-- Again, we adopted the same problem setting as in the paper [@yang2021b] -->\n<!-- to generate the training data, so that we can validate the results and -->\n<!-- for readers to compare them. The exact solution of $u$ is assumed to be -->\n<!-- $u=\\sin(\\pi x)\\sin(\\pi y)$ and the source $f$ can be calculated with the -->\n<!-- Equation [\\[2d ac equation\\]](#2d ac equation){reference-type=\"eqref\" -->\n<!-- reference=\"2d ac equation\"}. Further, we assume that 500 noisy -->\n<!-- measurements of $f$ are available in the domain which are randomly -->\n<!-- distributed. In addition, 25 equally spaced measurements of $u$ are -->\n<!-- available at each boundary. Again, we investigate two cases with -->\n<!-- different noise scales and the errors follow the same distributions as -->\n<!-- those in § [3.1.1](#linear case){reference-type=\"ref\" -->\n<!-- reference=\"linear case\"}. -->\n<!-- Figure [\\[fig:2d ac\\]](#fig:2d ac){reference-type=\"ref\" -->\n<!-- reference=\"fig:2d ac\"} shows the exact solution $u$ to the problem, the -->\n<!-- source $f$ and their respective measurements. The training data is -->\n<!-- generated in the same fashion as in the one-dimensional cases to provide -->\n<!-- the outputs of the neural networks, and the goal is also to find the -->\n<!-- posterior distribution of $u$. -->\n<!---->\n<!-- For both cases (small and large noise), the same neural network -->\n<!-- structure is used for $u$ and $f$, i.e. 3 hidden layers consisting of -->\n<!-- 200 neurons in each followed by the output layer with 2000 neurons. The -->\n<!-- ADAM optimizer is used for training with a learning rate of $10^{-3}$ -->\n<!-- and the results are collected after 50000 epochs. -->\n<!---->\n<!-- The results of the two-dimensional forward solution to the Allen-Cahn -->\n<!-- problem are shown in -->\n<!-- Figure [\\[fig:2d ac 0.01 noise\\]](#fig:2d ac 0.01 noise){reference-type=\"ref\" -->\n<!-- reference=\"fig:2d ac 0.01 noise\"} and Figure -->\n<!-- [\\[fig:2d ac 0.1 noise\\]](#fig:2d ac 0.1 noise){reference-type=\"ref\" -->\n<!-- reference=\"fig:2d ac 0.1 noise\"}. Similarly, the exact solution to the -->\n<!-- problem is largely bounded by two standard deviations for both cases -->\n<!-- with different scales of measurement noise. In addition, the error -->\n<!-- between the exact solution and the mean prediction grows as the noise -->\n<!-- scale increases. -->\n<!---->\n<!-- ## Inverse PDE problems -->\n<!---->\n<!-- We define *inverse problems* as those where all or some constitutive -->\n<!-- parameters are unknown and we are seeking complete solutions to the PDE -->\n<!-- and the discovery of the unknown parameters. Examples for linear and -->\n<!-- nonlinear problems in one and two dimensions are demonstrated. -->\n<!---->\n<!-- ### 1D diffusion-reaction system with nonlinear source term -->\n<!---->\n<!-- Again, we use the same problem as in the paper [@yang2021b] to -->\n<!-- demonstrate the capability of MO-PINN to solve inverse problems. Here, -->\n<!-- the PDE used in this section is the same as -->\n<!-- [\\[1d nonlinear possion equation\\]](#1d nonlinear possion equation){reference-type=\"eqref\" -->\n<!-- reference=\"1d nonlinear possion equation\"} except that this time the -->\n<!-- coefficient $k$ is assumed to be unknown and the goal is to find the -->\n<!-- corresponding distribution of $k$ with respect to noisy measurements of -->\n<!-- $u$ and $f$. We assume that 32 measurements of $f$ are available and -->\n<!-- equally sampled between $x \\in [-0.7, 0.7]$. Additionally, 8 -->\n<!-- measurements of $u$ are also available and equally sampled in same -->\n<!-- domain, and the solution of $u$ is assumed to be the same as in -->\n<!-- § [3.1.2](#1d nonlinear case){reference-type=\"ref\" -->\n<!-- reference=\"1d nonlinear case\"}. $k=0.7$ is considered to be the true -->\n<!-- solution of $k$ and used to generate the data for $f$. We also test our -->\n<!-- approach on cases with two noise scales which are the same as in -->\n<!-- previous sections. -->\n<!---->\n<!-- The neural network structures as well as the training setting remain the -->\n<!-- same as in the one-dimensional forward problems. In addition to the -->\n<!-- neural networks, a trainable array is defined for $k$ which corresponds -->\n<!-- to the multiple outputs of $u_{NN}$ and $f_{NN}$ when formulating the -->\n<!-- loss function. The array of $k$ is initialized with random values at the -->\n<!-- beginning of training, so that it does not require a priori information -->\n<!-- regarding the $k$ distribution. -->\n<!---->\n<!-- A summary of results of this one-dimensional inverse problem for both -->\n<!-- noise scale cases is shown in -->\n<!-- Figure [\\[fig:1d inverse\\]](#fig:1d inverse){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d inverse\"}. Similar to the results in the forward -->\n<!-- problems, the mean predictions of $u$ and $f$ pass through the noisy -->\n<!-- measurements. In addition, we end up with distributions of $k$ which is -->\n<!-- the objective of the inverse problem training. It is shown that the -->\n<!-- values in the array of $k$ follow a normal distribution as expected. -->\n<!-- Moreover, the true solution $k=0.7$ is bounded by the mean prediction of -->\n<!-- $k$ plus/minus two standard deviations which indicates that MO-PINN is -->\n<!-- capable of returning a reasonable posterior distribution in this inverse -->\n<!-- problem. -->\n<!---->\n<!-- ::: center -->\n<!-- ::: {#tab:1d inverse consistency} -->\n<!--   **No.**    **Mean**    **Std** -->\n<!--   --------- ---------- --------- -->\n<!--   1          $0.673$       0.073 -->\n<!--   2          $0.681$       0.075 -->\n<!--   3          $0.675$       0.072 -->\n<!--   4          $0.676$       0.073 -->\n<!--   5          $0.674$       0.071 -->\n<!--   6          $0.675$       0.073 -->\n<!---->\n<!--   : Mean and standard deviation of predicted $k$ of 6 simulations with -->\n<!--   different random seeds for case 2. -->\n<!-- ::: -->\n<!---->\n<!-- []{#tab:1d inverse consistency label=\"tab:1d inverse consistency\"} -->\n<!-- ::: -->\n<!---->\n<!-- We also tested the consistency of our solutions by training on the same -->\n<!-- measurements but different random seeds to initialize the neural -->\n<!-- networks. Results of 6 simulations are shown in -->\n<!-- Table [1](#tab:1d inverse consistency){reference-type=\"ref\" -->\n<!-- reference=\"tab:1d inverse consistency\"}, and it demonstrates that all -->\n<!-- return all of the distributions of predicted $k$ are in agreement. -->\n<!---->\n<!-- Additionally, we solved the problem with different number of outputs for -->\n<!-- the neural networks and the respective $k$ distributions are shown in -->\n<!-- Figure [\\[fig:1d inverse convergence\\]](#fig:1d inverse convergence){reference-type=\"ref\" -->\n<!-- reference=\"fig:1d inverse convergence\"}. It gives us a better intuition -->\n<!-- of the process that the posterior distribution forms as the number of -->\n<!-- outputs increases. -->\n<!---->\n<!-- ### Two-dimensional nonlinear diffusion-reaction system -->\n<!---->\n<!-- For the two-dimensional inverse problem, we consider the following PDE, -->\n<!-- $$\\begin{gathered} -->\n<!--     \\lambda \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) + k u^2 = f, \\qquad x,y \\in [-1, 1], \\label{2d inverse equation}  -->\n<!-- \\end{gathered}$$ where the constant $\\lambda=0.01$ and $k$ is an unknown -->\n<!-- parameter to be solved for with noisy measurements of $u$ and $f$. The -->\n<!-- same exact solution of $u$ as in -->\n<!-- § [3.1.3](#2d forward section){reference-type=\"ref\" -->\n<!-- reference=\"2d forward section\"} is considered here, that -->\n<!-- $u=\\sin(\\pi x)\\sin(\\pi y)$. One hundred noisy measurements of $u$ and -->\n<!-- $f$ are assumed to be available across the domain. In addition, 25 -->\n<!-- equally spaced measurements of $u$ are also available at each boundary. -->\n<!-- $k=1$ is the exact solution in this example and used to generate the -->\n<!-- training data. We also test the MO-PINN on two cases with the different -->\n<!-- noise scales. The errors in the measurements follow the same -->\n<!-- distributions as in previous sections. The structure of neural networks -->\n<!-- and the hyperparameters for training are kept the same as in -->\n<!-- § [3.1.3](#2d forward section){reference-type=\"ref\" -->\n<!-- reference=\"2d forward section\"}. The only difference is that we have an -->\n<!-- additional trainable array for $k$ which is of the size 2000 -->\n<!-- corresponding to the number of outputs for the two neural networks. -->\n<!---->\n<!-- The resulting $k$ distributions for both cases are shown in -->\n<!-- Figure [\\[fig:2d inverse k\\]](#fig:2d inverse k){reference-type=\"ref\" -->\n<!-- reference=\"fig:2d inverse k\"}. Again, we have similar observations as in -->\n<!-- the previous section that the exact solution of $k$ is within the two -->\n<!-- standard deviations confidence interval. Also, as the error in the -->\n<!-- measurements grow, the standard deviation of the $k$ distribution -->\n<!-- increases. -->\n<!---->\n<!-- ## Incorporation of additional prior knowledge to improve the prediction -->\n<!---->\n<!-- In this section, we will show that the MO-PINN approach is capable of -->\n<!-- taking prior knowledge from different sources into consideration to -->\n<!-- improve the predictions. We take the problem in -->\n<!-- § [3.1.1](#linear case){reference-type=\"ref\" reference=\"linear case\"} -->\n<!-- with the large noise scale as example for demonstration. First, we solve -->\n<!-- the same problem with the finite element method using a Monte Carlo -->\n<!-- approach. This is a common way to infer a distribution with only forward -->\n<!-- models available. In this work, the FEniCS package [@alnaes2015fenics] -->\n<!-- is used as the forward solver. In order to solve the problem with the -->\n<!-- FEM, the full information of source $f$ needs to be provided or at least -->\n<!-- at all quadrature points. Here, we made a simple assumption that the -->\n<!-- source $f$ is a linear function between measurements for the FEM -->\n<!-- approach. Therefore, for each set of training data of $u$ and $f$ in -->\n<!-- Section [3.1.1](#linear case){reference-type=\"ref\" -->\n<!-- reference=\"linear case\"}, there is a corresponding FEM solution to the -->\n<!-- problem, and they form the final posterior distributions as we did -->\n<!-- before. This can also be seen as another validation of the MO-PINN -->\n<!-- approach in solving this 1D problem. -->\n<!---->\n<!-- In Figure [\\[fig:fenics comp\\]](#fig:fenics comp){reference-type=\"ref\" -->\n<!-- reference=\"fig:fenics comp\"}, it shows the comparison of results between -->\n<!-- MO-PINN and the FEM Monte Carlo approach. The same number of forward -->\n<!-- simulations (500) were run to generate the posterior distributions for -->\n<!-- comparison. -->\n<!---->\n<!-- The resulting distributions of $u$ are almost the same and the minor -->\n<!-- differences are caused by different assumptions on $f$ in each approach. -->\n<!-- In MO-PINN, the neural networks naturally returns a smooth function -->\n<!-- interpolation of the measurements while we enforce $f$ to be linear in -->\n<!-- between for the FEM simulation. In -->\n<!-- Figure [\\[fig:quantile\\]](#fig:quantile){reference-type=\"ref\" -->\n<!-- reference=\"fig:quantile\"}, it shows another comparison of the two -->\n<!-- distributions of $u$ with a quantile-quantile plot. This type of plot is -->\n<!-- used to answer the question, \"What percentage of the data lies in the -->\n<!-- $x$ quantile of the prediction distribution?\\\" where, $x$ is -->\n<!-- $10\\%, 20\\%, \\ldots, 100\\%$. -->\n<!---->\n<!-- Next, we assume that only 5 measurements of $f$ are available and other -->\n<!-- settings remain the same. It can be seen in -->\n<!-- Figure [\\[fig:integration improve\\]](#fig:integration improve){reference-type=\"ref\" -->\n<!-- reference=\"fig:integration improve\"} that the data is not sufficient to -->\n<!-- provide us with reasonable predictions of $u$ and $f$. In practice, this -->\n<!-- is quite common that the direct measurements are expensive and sparse so -->\n<!-- that numerical simulations are usually performed to assist the -->\n<!-- predictions. Here we assume that the mean and standard deviation of $u$ -->\n<!-- from the FEM approach at the 9 locations in -->\n<!-- Figure [\\[fig:quantile\\]](#fig:quantile){reference-type=\"ref\" -->\n<!-- reference=\"fig:quantile\"} are also available to us and we include this -->\n<!-- additional knowledge into our training to improve our predictions. -->\n<!---->\n<!-- In -->\n<!-- Figure [\\[fig:integration improve\\]](#fig:integration improve){reference-type=\"ref\" -->\n<!-- reference=\"fig:integration improve\"}, it shows that the predictions of -->\n<!-- $u$ and $f$ are highly improved with using mean and standard deviation -->\n<!-- information. Furthermore, it can be seen that the predictions can also -->\n<!-- be improved if only the means are available. In the framework of -->\n<!-- MO-PINN, this integration of additional data can be easily implemented -->\n<!-- by modifying the loss function which makes the approach suitable for the -->\n<!-- scenarios where the sources of data are varied and complicated. -->\n<!---->\n<!-- # Summary -->\n<!---->\n<!-- We proposed the MO-PINNs in this work to solve both forward and inverse -->\n<!-- PDE based problems with noisy data. This approach allowed us to use -->\n<!-- prior knowledge of statistical distributions into the calculations under -->\n<!-- the framework of PINNs. Our numerical results indicate that the MO-PINNs -->\n<!-- can return accurate distributions at the end of training for both -->\n<!-- forward and inverse problems. Finally, for the first time, we compared -->\n<!-- the the predicted distributions using pure deep learning framework with -->\n<!-- solutions from the traditional numerical solver (FEMs). We also -->\n<!-- demonstrated that our approach is capable of using limited statistical -->\n<!-- knowledge of the data to make a better prediction. -->\n<!---->\n<!-- In the future, we would like to advance this approach on problems with -->\n<!-- multi-fidelity data which is common in the engineering applications. We -->\n<!-- are also interested in using this deep learning framework with practical -->\n<!-- conventional PDE solvers in real engineering applications which are -->\n<!-- usually very computationally expensive, with the goal of making the -->\n<!-- ensemble of uncertain predictions faster. Finally, we are also -->\n<!-- interested in extending this approach to time-dependent problems. From -->\n<!-- the view of implementation, the time dimension is the similar to the -->\n<!-- space dimensions; however, a different behavior of the uncertainty -->\n<!-- correspondence may appear as the early uncertainties are propagated and -->\n<!-- amplified over time. -->\n",
    "supporting": [
      "usnccm17_mopinn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}